{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "H142IMBXP42p"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m33shf4Ubq9e"
   },
   "source": [
    "# model to env\n",
    "import shutil\n",
    "shutil.unpack_archive('/content/drive/MyDrive/Colab Notebooks/merge268files_1/model_checkpoint_0.47_v15k-SL64-B400-6-14.tar.gz','model_checkpoint_0.47')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 5226,
     "status": "ok",
     "timestamp": 1721873539467,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "ueZipgI0V08d",
    "outputId": "37b6ff1d-80a8-4550-9ffb-08d83b74a883"
   },
   "source": [
    "# model to drive\n",
    "import shutil\n",
    "shutil.make_archive('/content/drive/MyDrive/Colab Notebooks/merge268files_1/model_checkpoint_0.36_v15k-SL256-B200-6-14', 'gztar', 'model_checkpoint_0.36')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1725878062042,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "CfQf_iro7fj8",
    "outputId": "06359ab3-1d1c-45b9-d060-bb6b61982330"
   },
   "source": [
    "# bpe_tokenizer to env\n",
    "import shutil\n",
    "shutil.copy('/content/drive/MyDrive/Colab Notebooks/merge268files_1/bpe_tokenizer.json', 'bpe_tokenizer.json')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ib-r2DB4oK8h"
   },
   "source": [
    "# bpe_tokenizer to drive\n",
    "import shutil\n",
    "shutil.copy('bpe_tokenizer.json', '/content/drive/MyDrive/Colab Notebooks/merge268files_1/bpe_tokenizer_new15k.json')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 4719,
     "status": "ok",
     "timestamp": 1725878076251,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "QMiVr99F8M5x",
    "outputId": "c003599c-ad7b-48ca-9790-aecef1a59ffc"
   },
   "source": [
    "# tokens to env\n",
    "import shutil\n",
    "shutil.copy('/content/drive/MyDrive/Colab Notebooks/merge268files_1/tokens_15k.npy', 'tokens.npy')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q1ZaMeljbc4P"
   },
   "source": [
    "# tokens to drive\n",
    "import shutil\n",
    "shutil.copy('tokens.npy', '/content/drive/MyDrive/Colab Notebooks/merge268files_1/tokens_new15k.npy')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y9zf02bhbOku"
   },
   "source": [
    "# data_file to env\n",
    "import shutil\n",
    "shutil.copy('/content/drive/MyDrive/Colab Notebooks/merge268files_1/merge268files.txt', 'merge268files.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9zEiDAKsuLea"
   },
   "source": [
    "my_data_file = 'merge70files.txt'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install tokenizers",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-MI0gZpNnyGD"
   },
   "source": [
    "# СОЗДАНИЕ СЛОВАРЯ\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "import time\n",
    "\n",
    "# Создание модели BPE\n",
    "bpe_model = models.BPE()\n",
    "\n",
    "# Настройка токенизатора\n",
    "tokenizer = Tokenizer(bpe_model)\n",
    "\n",
    "# Добавление специальных токенов для пробелов и других символов\n",
    "special_tokens = [\"[END]\", \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<space>\"]\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Использование пробельного пре-токенизатора\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "# Настройка декодера для корректного восстановления текста\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Обучение токенизатора на корпусе данных\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=15000,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=True\n",
    ")\n",
    "files = [my_data_file]\n",
    "startt = time.time()\n",
    "tokenizer.train(files, trainer)\n",
    "print(time.time()-startt)\n",
    "\n",
    "# Тестирование токенизатора\n",
    "input_text = 'мое времяпровождение заключается в том чтобы создавать нейронные сети'\n",
    "tokens = tokenizer.encode(input_text).ids\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "\n",
    "print(\"Original text:\", input_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "# Сохранение токенизатора\n",
    "tokenizer.save(\"bpe_tokenizer.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_YON2k_ayyoR"
   },
   "source": [
    "# СОЗДАНИЕ СЛОВАРЯ new\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "import time\n",
    "\n",
    "# Создание модели BPE\n",
    "bpe_model = models.BPE()\n",
    "\n",
    "# Настройка токенизатора\n",
    "tokenizer = Tokenizer(bpe_model)\n",
    "\n",
    "# Добавление специальных токенов\n",
    "special_tokens = [\"[END]\", \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<space>\"]\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Использование пробельного пре-токенизатора\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "# Настройка пост-процессора для улучшенной сегментации\n",
    "post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "tokenizer.post_processor = post_processor\n",
    "\n",
    "# Настройка декодера для корректного восстановления текста\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Обучение токенизатора на корпусе данных\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=15000,\n",
    "    #min_frequency=2,\n",
    "    special_tokens=special_tokens,\n",
    "    #show_progress=True,\n",
    "    #continuing_subword_prefix=\"##\",\n",
    "    #end_of_word_suffix=\"##\",\n",
    "    dropout=0.1\n",
    ")\n",
    "files = [my_data_file]  # Убедитесь, что путь к файлу данных верен\n",
    "startt = time.time()\n",
    "tokenizer.train(files, trainer)\n",
    "print(f\"Training time: {time.time()-startt:.2f} seconds\")\n",
    "\n",
    "# Тестирование токенизатора\n",
    "input_text = 'мое времяпровождение заключается в том чтобы создавать нейронные сети'\n",
    "encoded = tokenizer.encode(input_text)\n",
    "tokens = encoded.ids\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "\n",
    "print(\"Original text:\", input_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "# Сохранение токенизатора\n",
    "tokenizer.save(\"bpe_tokenizer.json\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P7wcg8FUwN1x"
   },
   "source": [
    "my_model_path = 'models/model_checkpoint_0.20.keras'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install ipywidgets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "get_custom_objects().update({'TransformerBlock': TransformerBlock})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "7c9efc836ec243a4b8010ce2acf73211",
      "92adcd3a0313485cb6236767c482346c",
      "93af37169f7d496dbb8eedcbe109c31b",
      "4096054553e24369a63f677390bffe2b",
      "42abb3194f884dfea0ef6473929b50f2",
      "2c5c3fd6e6524e22beed5bb190f6bc63",
      "1cdaaa6cbc444614b12355515f6bc07e",
      "e6bd956b412b4830b68d3ee3a0c9f90b",
      "74e88459195a471fae84e01e2d59f60d",
      "4adfe569485f463a880d339af1e4c34e",
      "97c1e034ac4c4dfe93b022a08d5c8924",
      "2c7694c0adda4b92be614a43e06a5884",
      "bb324439258a43b0bb297139cfece28d",
      "c98fe97d14e048378cb455575f9d4449",
      "27aa2b10c58b49c08805a25d4cb99089",
      "35c13ffbd0cd45d4905e49ed8b902b70",
      "7062a8d2a73a4e8b8e708f24814b7244"
     ]
    },
    "executionInfo": {
     "elapsed": 13170,
     "status": "ok",
     "timestamp": 1720256040464,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "k3ZcoBaAPsGH",
    "outputId": "3f99e488-774c-4e95-c59c-74c8e0cc0b28"
   },
   "source": [
    "# ОБЩЕНИЕ С СЕТОЧКОЙ\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Загружаем модель и токенизатор\n",
    "dir_path = ''\n",
    "tokenizer = Tokenizer.from_file(dir_path + my_tokenizer_path)\n",
    "model_path = dir_path + my_model_path\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "#model.summary()\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def tail_free_sampling(logits, tail_free_threshold=0.9, temperature=1.0):\n",
    "    # Apply temperature scaling\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Convert logits into probabilities using softmax\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    # Sort probabilities in descending order\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    sorted_probs = np.sort(probs)[::-1]\n",
    "\n",
    "    # Calculate the first derivative\n",
    "    first_derivative = np.diff(sorted_probs)\n",
    "\n",
    "    # Calculate the second derivative\n",
    "    second_derivative = np.diff(first_derivative)\n",
    "\n",
    "    # Calculate the absolute values of the second derivative\n",
    "    abs_second_derivative = np.abs(second_derivative)\n",
    "\n",
    "    # Normalize the second derivative\n",
    "    normalized_second_derivative = abs_second_derivative / np.sum(abs_second_derivative)\n",
    "\n",
    "    # Find the smallest subset of second derivative magnitudes that surpass the threshold\n",
    "    cumulative_sum = np.cumsum(normalized_second_derivative)\n",
    "    cutoff_index = np.searchsorted(cumulative_sum, tail_free_threshold, side='right')\n",
    "\n",
    "    # Keep only tokens up to the cutoff index\n",
    "    if cutoff_index < len(sorted_probs):\n",
    "        sorted_indices = sorted_indices[:cutoff_index + 1]\n",
    "        sorted_probs = sorted_probs[:cutoff_index + 1]\n",
    "    else:\n",
    "        sorted_indices = sorted_indices[:1]\n",
    "        sorted_probs = sorted_probs[:1]\n",
    "\n",
    "    # Renormalize probabilities\n",
    "    normalized_probs = sorted_probs / np.sum(sorted_probs)\n",
    "\n",
    "    # Sample next token based on the normalized probabilities\n",
    "    next_token = np.random.choice(sorted_indices, p=normalized_probs)\n",
    "\n",
    "    return next_token\n",
    "\n",
    "def generate_text(model, tokenizer, input_text, max_length=100, temperature=1.0, tail_free_threshold=0.9):\n",
    "    tokens = tokenizer.encode(input_text).ids\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        token_array = np.array(tokens)[None, :]  # Add batch dimension\n",
    "        predictions = model.predict(token_array, verbose=0)\n",
    "        next_token_logits = predictions[0, -1, :]\n",
    "\n",
    "        next_token = tail_free_sampling(next_token_logits, tail_free_threshold=tail_free_threshold, temperature=temperature)\n",
    "        tokens.append(next_token)\n",
    "\n",
    "        # Отображаем ответ с добавлением новых токенов в той же строке\n",
    "        #print(f\"\\rText: {tokenizer.decode(tokens)}\", end='', flush=True)\n",
    "\n",
    "        if next_token == tokenizer.token_to_id(\"[END]\"):\n",
    "            break\n",
    "        if ')' in tokenizer.decode([next_token]):\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "# Define generation parameters\n",
    "temperature = 0.5\n",
    "tail_free_threshold = 0.995\n",
    "max_length = 50\n",
    "\n",
    "# Взаимодействие с моделью\n",
    "while False:\n",
    "    #input_text = input(\"\\nYou: \")\n",
    "    input_text = my_input_text\n",
    "    if input_text.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    response = generate_text(model, tokenizer, input_text, max_length=max_length, temperature=temperature, tail_free_threshold=tail_free_threshold)\n",
    "    print(f\"AI:{response}\")\n",
    "#for i in range(5,15+1, 2):\n",
    "#    for j in range(5, 15+1, 2):\n",
    "#        print(f\"\\ntemp={i*0.1} tfs={j*0.1}\")\n",
    "#        print(generate_text(model, tokenizer, \"такой кайф\", max_length=max_length, temperature=i * 0.1, tail_free_threshold=j * 0.1))\n",
    "\n",
    "# Создаем виджеты\n",
    "text_input = widgets.Textarea(\n",
    "    value='Артем: ',\n",
    "    placeholder='Введите текст...',\n",
    "    description='AI:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='200px')  # Устанавливаем ширину и высоту\n",
    ")\n",
    "\n",
    "max_length_slider = widgets.IntSlider(\n",
    "    value=max_length,\n",
    "    min=1,\n",
    "    max=201,\n",
    "    step=5,\n",
    "    description='Max length:',\n",
    "    continuous_update=False\n",
    ")\n",
    "temperature_slider = widgets.FloatSlider(\n",
    "    value=temperature,\n",
    "    min=0.1,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description='Temperature:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# Создаем логарифмический слайдер для tail_free_threshold\n",
    "#log_scale = np.logspace(np.power(2,np.log10(0.05)), np.power(2, np.log10(1.0)), num=100)\n",
    "\n",
    "tail_free_slider = widgets.FloatSlider(\n",
    "    value=np.power(10, np.log10(tail_free_threshold)/np.log10(2)),\n",
    "    min=0.05,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='Tail Free:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        prompt = text_input.value\n",
    "        response = generate_text(model, tokenizer, prompt, max_length=max_length, temperature=temperature, tail_free_threshold=tail_free_threshold)\n",
    "        text_input.value += response[len(text_input.value)+1:] + \"\\nАртем: \"\n",
    "        #print(f\"\\r{response}\", end='', flush=True)\n",
    "        print(temperature, tail_free_threshold, max_length)\n",
    "\n",
    "button = widgets.Button(description=\"Сгенерировать текст\")\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "def update_params(change):\n",
    "    global temperature\n",
    "    global tail_free_threshold\n",
    "    global max_length\n",
    "    temperature = temperature_slider.value\n",
    "    tail_free_threshold = np.power(1.25,np.log10(tail_free_slider.value))\n",
    "    max_length = max_length_slider.value\n",
    "    #print(f\"\\r{tail_free_threshold}\", end='', flush=True)\n",
    "\n",
    "temperature_slider.observe(update_params, names='value')\n",
    "tail_free_slider.observe(update_params, names='value')\n",
    "\n",
    "# Отображаем виджеты\n",
    "display(text_input)\n",
    "display(button)\n",
    "display(temperature_slider)\n",
    "display(tail_free_slider)\n",
    "display(max_length_slider)\n",
    "display(output_area)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sZqq5WMrwzGD"
   },
   "source": [
    "my_data_file2 = my_data_file"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E31UUxRhxeC_"
   },
   "source": [
    "my_tokenizer_path2 = 'bpe_tokenizer.json'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install tensorflow",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FBA7eAD90K5v"
   },
   "source": [
    "# ТОКЕНИЗАЦИЯ ДАТА ФАЙЛА\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "import os\n",
    "import time\n",
    "\n",
    "dir_path = ''\n",
    "data_file = my_data_file2\n",
    "\n",
    "print(\"Загрузка BPE-токенизатора\")\n",
    "tokenizer = Tokenizer.from_file(dir_path + my_tokenizer_path2)\n",
    "\n",
    "# Получение словаря из токенизатора\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Вокабуляр содержит {vocab_size} слов\")\n",
    "\n",
    "# Функция для токенизации и записи токенов в файл\n",
    "def tokenize_and_save(file_path, chunk_size=50000, token_file_path=dir_path + 'tokens.npy'):\n",
    "    token_count = 0\n",
    "    with open(file_path, 'r') as f:\n",
    "        if os.path.exists(token_file_path):\n",
    "            tokens = list(np.load(token_file_path))\n",
    "            token_count = len(tokens)\n",
    "        else:\n",
    "            tokens = []\n",
    "\n",
    "        while True:\n",
    "            text_chunk = f.read(chunk_size)\n",
    "            if not text_chunk:\n",
    "                break\n",
    "            tokens_chunk = tokenizer.encode(text_chunk).ids\n",
    "            tokens.extend(tokens_chunk)\n",
    "            token_count += len(tokens_chunk)\n",
    "            print(f\"\\rТокенов: {token_count}\", end='', flush=True)\n",
    "\n",
    "        print()\n",
    "        np.save(token_file_path, tokens)\n",
    "        return token_count\n",
    "\n",
    "if os.path.exists(dir_path + 'tokens.npy'):\n",
    "    os.remove(dir_path + 'tokens.npy')\n",
    "\n",
    "print(\"Токенизация и сохранение токенов в файл\")\n",
    "token_count = tokenize_and_save(dir_path + data_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W6BPumv9d9uo"
   },
   "source": [
    "token_count = 51042083"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hj-3d6SO3HQQ"
   },
   "source": [
    "token_count = 30673990"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "token_count = 22682187"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1725880529416,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "iqldKJHgvvsd"
   },
   "source": [
    "my_tokenizer_path = 'bpe_tokenizer.json'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3408,
     "status": "ok",
     "timestamp": 1725880533974,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "VVrfWhc8yMwS",
    "outputId": "e76e191f-9ec5-4bb0-e490-b34d5478b12f"
   },
   "source": [
    "# ИМПОРТЫ БЕЗ ТОКЕНИЗАЦИИ ДАТА ФАЙЛА\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import keras\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "print(\"Загрузка BPE-токенизатора\")\n",
    "tokenizer = Tokenizer.from_file(my_tokenizer_path)\n",
    "\n",
    "# Получение словаря из токенизатора\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Вокабуляр содержит {vocab_size} слов\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1725880433043,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "-ex3UoymXhfJ"
   },
   "source": [
    "from transformers import GPTNeoXForCausalLM\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m-deduped\",\n",
    "  revision=\"step143000\",\n",
    "  cache_dir=\"./pythia-70m-deduped/step143000\",\n",
    ")\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p pythia\n",
    "model.save_pretrained('pythia')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tsEIM749dmko"
   },
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "dir = \"pythia\"\n",
    "\n",
    "model = TFAutoModel.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m-deduped\",\n",
    "  revision=\"step143000\",\n",
    "  cache_dir=\"./pythia-70m-deduped/step143000\",)\n",
    "model.save(dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4455,
     "status": "ok",
     "timestamp": 1725882064387,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "33z2uiuoj1LZ",
    "outputId": "ec11cd32-a528-4977-9dac-1c862deb2696"
   },
   "source": [
    "from transformers import AutoModelForCausalLM, TFAutoModelForCausalLM\n",
    "#import tensorflow as tf\n",
    "\n",
    "# Шаг 1: Загрузить модель PyTorch\n",
    "torch_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"pythia\",\n",
    "    from_tf=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1006,
     "status": "ok",
     "timestamp": 1725882116908,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "aA-8B2VYlvz_"
   },
   "source": [
    "# Шаг 2: Сохраните весы модели в PyTorch формате (для безопасности)\n",
    "torch_model.save_pretrained(\"./torch_model\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "error",
     "timestamp": 1725882185429,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "1uZ7Q9E3l7IY",
    "outputId": "c4f3f96a-b023-4d1f-f26e-437d9258feaf"
   },
   "source": [
    "from transformers import TFAutoModelForCausalLM\n",
    "# Шаг 3: Загрузите модель TensorFlow\n",
    "tf_model = TFAutoModelForCausalLM.from_pretrained(\n",
    "    \"./torch_model\",\n",
    "    from_pt=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1725880541127,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "Ed_3BGRste5l",
    "outputId": "7b49ae22-ad72-4999-e10b-38800001eb25"
   },
   "source": [
    "# ЗАГРУЗКА ДАТА ФАЙЛА В RAM\n",
    "\n",
    "dir_path = ''\n",
    "\n",
    "# Загрузка токенов из файла\n",
    "def load_tokens(token_file_path):\n",
    "    tokens = np.load(token_file_path)\n",
    "    return tokens\n",
    "\n",
    "tokens = load_tokens(dir_path + 'tokens.npy')\n",
    "token_count = len(tokens)\n",
    "print(f\"Загружено {token_count} токенов\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1725880544684,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "zfQvTviY3SOB"
   },
   "source": [
    "create_nn = True\n",
    "nn_file = 'model_checkpoint_0.41'\n",
    "#nn_file = 'pythia'\n",
    "model_scale = 4 # масштаб блока трансформера\n",
    "num_transformer_blocks = 4 # количество блоков трансформера\n",
    "use_lr_scheduler = False\n",
    "initial_lr = 0.001 #1e-7 #0.001\n",
    "\n",
    "sequence_length = 8 # размер сэмпла\n",
    "batch_size = 4*50 #8*50 #int(150000/sequence_length) # размер батча\n",
    "\n",
    "epochs = 1 #5\n",
    "epochs_infit = 1 #5\n",
    "# настройте это значение в зависимости от вашего датасета\n",
    "steps_per_epoch = 50 #500 #15 #for lr_scheduler #500 #token_count // (batch_size * sequence_length)\n",
    "save_freq= 50 #5000 #'epoch'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 25855,
     "status": "error",
     "timestamp": 1725880573579,
     "user": {
      "displayName": "Артем Акбатыров",
      "userId": "12293389224183551040"
     },
     "user_tz": -180
    },
    "id": "PvyiRG1UOM8P",
    "outputId": "62dae2d5-b5f1-43c7-b564-1156517fa553"
   },
   "source": [
    "# ОБУЧЕНИЕ СЕТИ\n",
    "\n",
    "# Проверка и инициализация TPU\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    if 'strategy' not in globals():\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    print('Running on TPU')\n",
    "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Running on default strategy (CPU/GPU)')\n",
    "\n",
    "class PrintLearningRate(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.learning_rate.numpy()\n",
    "        print(f\"\\nLearning rate after epoch {epoch+1}: {lr:.4e}\")\n",
    "\n",
    "# Функция для разогрева learning rate\n",
    "def lr_scheduler(epoch):\n",
    "    warmup_epochs = 10  # Количество эпох для разогрева\n",
    "    if epoch < warmup_epochs:\n",
    "        return initial_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        return initial_lr #* tf.math.exp(-0.1 * (epoch - warmup_epochs))\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, trainable=True, dtype='float32', rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "tf.keras.utils.get_custom_objects().update({'TransformerBlock': TransformerBlock})\n",
    "\n",
    "def create_model(create_nn, model_scale, num_transformer_blocks, nn_file, vocab_size):\n",
    "    if create_nn:\n",
    "        embed_dim = 16 * model_scale\n",
    "        num_heads = 1 * model_scale\n",
    "        ff_dim = 16 * model_scale\n",
    "\n",
    "        inputs = tf.keras.Input(shape=(None,))\n",
    "        embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        x = embedding_layer(inputs)\n",
    "        for _ in range(num_transformer_blocks):\n",
    "            transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            x = transformer_block(x, training=True)\n",
    "        outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        model = tf.keras.models.load_model(nn_file)\n",
    "    return model\n",
    "\n",
    "# RAM\n",
    "# Функция для генерации батчей из массива токенов\n",
    "def generate_batch_ram(tokens, batch_size, sequence_length):\n",
    "    indices = np.random.randint(0, len(tokens) - sequence_length - 1, batch_size)\n",
    "    X = np.zeros((batch_size, sequence_length), dtype=np.int64)\n",
    "    Y = np.zeros((batch_size, sequence_length), dtype=np.int64)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        X[i] = tokens[idx:idx + sequence_length]\n",
    "        Y[i] = tokens[idx + 1:idx + 1 + sequence_length]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def generate_dataset_ram(tokens, batch_size, sequence_length):\n",
    "    def generator():\n",
    "        while True:\n",
    "            yield generate_batch_ram(tokens, batch_size, sequence_length)\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(batch_size, sequence_length), dtype=tf.int64),\n",
    "            tf.TensorSpec(shape=(batch_size, sequence_length), dtype=tf.int64)\n",
    "        )\n",
    "    )\n",
    "    return dataset.repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "distributed_dataset = strategy.distribute_datasets_from_function(\n",
    "    lambda _: generate_dataset_ram(tokens, batch_size, sequence_length)\n",
    ")\n",
    "\n",
    "print(f\"Создать модель - {create_nn}\")\n",
    "with strategy.scope():\n",
    "    if 'model' in globals():\n",
    "        del model\n",
    "        gc.collect()\n",
    "    model = create_model(create_nn, model_scale, num_transformer_blocks, dir_path + nn_file, vocab_size)\n",
    "\n",
    "#model.summary()  # вывод информации о модели\n",
    "\n",
    "# Настройка callback для сохранения модели\n",
    "checkpoint_filepath = 'model_checkpoint_{accuracy:.2f}.keras'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_freq=save_freq,#int(50000/sequence_length),#int(832000000 / (model_scale * num_transformer_blocks * batch_size * vocab_size * sequence_length)),  # настраиваемое значение\n",
    "    #save_format='tf'  # Использование формата TensorFlow SavedModel\n",
    ")\n",
    "#print(f'save_freq = {int(832000000 / (model_scale * num_transformer_blocks * batch_size * vocab_size * sequence_length))}')\n",
    "print(f'save_freq = {int(50000/sequence_length)}')\n",
    "\n",
    "# Настройка callback для разогрева learning rate\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Добавление callback в список callbacks\n",
    "if use_lr_scheduler:\n",
    "    callbacks = [checkpoint_callback, lr_callback]\n",
    "else:\n",
    "    callbacks = [checkpoint_callback, PrintLearningRate()]\n",
    "\n",
    "print(\"Обучение модели\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Обучение модели с отображением прогресса\n",
    "print(f\"model_scale = {model_scale}, num_transformer_blocks = {num_transformer_blocks}\")\n",
    "print(f\"sequence = {sequence_length}, batch = {batch_size}\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    model.fit(\n",
    "        distributed_dataset,\n",
    "        epochs=epochs_infit,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"Total elapsed time: {minutes}:{seconds}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ru6phOIruBxE"
   },
   "source": [
    "# ДВА МЕТОДА БАТЧИНГА, RAM И ДИСК\n",
    "\n",
    "# ДИСК\n",
    "# Функция для генерации батчей из файла\n",
    "def generate_batch(file_path, batch_size, sequence_length):\n",
    "    indices = np.random.randint(0, token_count - sequence_length - 1, batch_size)\n",
    "    X = np.zeros((batch_size, sequence_length), dtype=np.int64)\n",
    "    Y = np.zeros((batch_size, sequence_length), dtype=np.int64)\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "            for i, idx in enumerate(indices):\n",
    "                # Чтение X\n",
    "                f.seek(idx * 8)  # np.int64 занимает 8 байт\n",
    "                x_seq = np.fromfile(f, dtype=np.int64, count=sequence_length)\n",
    "\n",
    "                # Чтение Y\n",
    "                f.seek((idx + 1) * 8)  # смещение на 1 элемент\n",
    "                y_seq = np.fromfile(f, dtype=np.int64, count=sequence_length)\n",
    "\n",
    "                if np.any(x_seq >= vocab_size) or np.any(y_seq >= vocab_size):\n",
    "                    print(f\"Found token with index >= {vocab_size}\")\n",
    "                    continue\n",
    "\n",
    "                X[i] = x_seq\n",
    "                Y[i] = y_seq\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# RAM\n",
    "# Функция для генерации батчей из массива токенов\n",
    "def generate_batch(tokens, batch_size, sequence_length):\n",
    "    indices = np.random.randint(0, len(tokens) - sequence_length - 1, batch_size)\n",
    "    X = np.zeros((batch_size, sequence_length), dtype=np.int64)\n",
    "    Y = np.zeros((batch_size, sequence_length), dtype=np.int64)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        X[i] = tokens[idx:idx + sequence_length]\n",
    "        Y[i] = tokens[idx + 1:idx + 1 + sequence_length]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# НЕ НАШЕЛ ПОЛЕЗНЫМ\n",
    "def batch_generator(file_path, batch_size, sequence_length, steps_per_epoch):\n",
    "    for _ in range(steps_per_epoch):\n",
    "        yield generate_batch(file_path, batch_size, sequence_length)\n",
    "\n",
    "\n",
    "# внутри цикла эпох\n",
    "#dataset = tf.data.Dataset.from_generator(\n",
    "    #    lambda: batch_generator(dir_path + 'tokens.npy', batch_size, sequence_length, steps_per_epoch),\n",
    "    #     lambda: generate_batch(dir_path + 'tokens.npy', batch_size, sequence_length),\n",
    "    #    output_signature=(\n",
    "    #        tf.TensorSpec(shape=(batch_size, sequence_length), dtype=tf.int64),\n",
    "    #        tf.TensorSpec(shape=(batch_size, sequence_length), dtype=tf.int64),\n",
    "    #    )\n",
    "    #)\n",
    "\n",
    "    # Применение параллельной обработки\n",
    "    #dataset = dataset.map(lambda x, y: (x, y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    #dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "# ДЛЯ TPU ИСПОЛЬЗУЙ TOKENS, не .npy\n",
    "dataset = tf.data.Dataset.from_generator(# 'tokens.npy'\n",
    "    lambda: iter([generate_batch(tokens, batch_size, sequence_length)]),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, sequence_length), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(batch_size, sequence_length), dtype=tf.int64),\n",
    "    )\n",
    ")\n",
    "dataset = dataset.repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "#tf.keras.backend.clear_session() # не помогло сбросить TPU\n",
    "try:\n",
    "    tpu_address = os.environ.get('COLAB_TPU_ADDR')\n",
    "    if tpu_address:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + tpu_address)\n",
    "    else:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "if tpu:\n",
    "    print(\"TPU = True\")\n",
    "else:\n",
    "    print(\"TPU = False\")\n",
    "if tpu:\n",
    "    try:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        if 'strategy' not in globals():\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "        print('Running on TPU')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize TPU: {e}\")\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "else:\n",
    "    # Если TPU недоступен, используем стратегию по умолчанию\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Running on default strategy (CPU/GPU)')\n",
    "\n",
    "\n",
    "if create_nn:\n",
    "    print(\"Создание модели\")\n",
    "    class TransformerBlock(layers.Layer):\n",
    "        def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "            super(TransformerBlock, self).__init__()\n",
    "            self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "            self.ffn = tf.keras.Sequential(\n",
    "                [layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                 layers.Dense(embed_dim),]\n",
    "            )\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, inputs, training):\n",
    "            attn_output = self.att(inputs, inputs)\n",
    "            attn_output = self.dropout1(attn_output, training=training)\n",
    "            out1 = self.layernorm1(inputs + attn_output)\n",
    "            ffn_output = self.ffn(out1)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    embed_dim = 16 * model_scale  # размерность вложения\n",
    "    num_heads = 1 * model_scale  # количество голов в механизме внимания\n",
    "    ff_dim = 16 * model_scale  # размерность полносвязного слоя\n",
    "    with strategy.scope():\n",
    "        inputs = tf.keras.Input(shape=(None,))\n",
    "        embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        x = embedding_layer(inputs)\n",
    "        for _ in range(num_transformer_blocks):\n",
    "            transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            x = transformer_block(x)\n",
    "        outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        print(\"Компиляция модели\")\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "else:\n",
    "    # Настройка модели внутри TPU стратегии\n",
    "    with strategy.scope():\n",
    "        print(\"Загрузка обученной модели\")\n",
    "        model = tf.keras.models.load_model(my_model_path)\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "\n",
    "! gcloud compute tpus locations list --project=project\n",
    "! gcloud compute tpus stop --zone=ZONE\n",
    "\n",
    "\n",
    "# Функция для поиска оптимального learning rate\n",
    "def find_optimal_lr(model, dataset, steps, init_value=1e-7, final_value=10, beta=0.98):\n",
    "    num = steps - 1\n",
    "    mult = (final_value / init_value) ** (1/num)\n",
    "    lr = init_value\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "\n",
    "    @tf.function\n",
    "    def step_fn(inputs, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = model.compiled_loss(targets, predictions)\n",
    "        return loss\n",
    "\n",
    "    for step, (inputs, targets) in enumerate(dataset.take(steps)):\n",
    "        print(f\"\\rFind_lr_step = {step+1}/{steps}\", end='', flush=True)\n",
    "\n",
    "        batch_num += 1\n",
    "        loss = strategy.run(step_fn, args=(inputs, targets))\n",
    "        loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, loss, axis=None)\n",
    "\n",
    "        avg_loss = beta * avg_loss + (1 - beta) * loss.numpy()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "\n",
    "        # здесь 4 это threshold, ловит взрыв loss в х4\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            break\n",
    "\n",
    "        if smoothed_loss < best_loss or batch_num == 1:\n",
    "            best_loss = smoothed_loss\n",
    "\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(np.log10(lr))\n",
    "\n",
    "        #grads = tape.gradient(loss, model.trainable_variables)\n",
    "        #optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        lr *= mult\n",
    "        optimizer.learning_rate.assign(lr)\n",
    "\n",
    "    slopes = [(losses[i+1] - losses[i]) / (log_lrs[i+1] - log_lrs[i]) for i in range(len(losses)-1)]\n",
    "    min_slope_index = np.argmin(slopes)\n",
    "    optimal_lr = 10**log_lrs[min_slope_index]\n",
    "\n",
    "    return optimal_lr\n",
    "\n",
    "\n",
    "# Находим оптимальное значение learning rate\n",
    "steps = 10  # Количество шагов для поиска оптимального learning rate\n",
    "init_value = 1e-7\n",
    "final_value = 1e-2\n",
    "dataset = generate_dataset_ram(tokens, batch_size, sequence_length)\n",
    "if find_lr:\n",
    "    optimal_lr = find_optimal_lr(model, dataset, steps, init_value=init_value, final_value=final_value)\n",
    "else:\n",
    "    optimal_lr = initial_lr\n",
    "print(f\"\\nOptimal Learning Rate: {optimal_lr:.4e}\")\n",
    "\n",
    "# Создаем оптимизатор с начальной learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=optimal_lr)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bs1kBjPgxGYF"
   },
   "source": [
    "# БЕНЧМАРК\n",
    "\n",
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "# Функция для вычисления чисел Фибоначчи\n",
    "def fibonacci(n):\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n - 1) + fibonacci(n - 2)\n",
    "\n",
    "# Функция для выполнения матричных операций с NumPy\n",
    "def matrix_operations(size):\n",
    "    A = np.random.rand(size, size)\n",
    "    B = np.random.rand(size, size)\n",
    "    C = np.dot(A, B)\n",
    "    return C\n",
    "\n",
    "# Количество итераций для измерений\n",
    "iterations = 10\n",
    "fibonacci_time = 0\n",
    "matrix_time = 0\n",
    "for i in range(10):\n",
    "    # Измерение времени выполнения вычислений чисел Фибоначчи\n",
    "    fibo = 30\n",
    "    fibonacci_time = (fibonacci_time*i + timeit.timeit(\"fibonacci(fibo)\", globals=globals(), number=iterations))/(i+1)\n",
    "    print(f\"Время выполнения вычислений чисел Фибоначчи: {fibonacci_time / iterations:.6f} секунд на итерацию\")\n",
    "\n",
    "    # Измерение времени выполнения матричных операций\n",
    "    matrix_size = 1600\n",
    "    matrix_time = (matrix_time*i + timeit.timeit(\"matrix_operations(matrix_size)\", globals=globals(), number=iterations))/(i+1)\n",
    "    print(f\"Время выполнения матричных операций: {matrix_time / iterations:.6f} секунд на итерацию\\n\")\n",
    "\n",
    "# Создание файла с результатами\n",
    "with open(\"benchmark_results.txt\", \"w\") as f:\n",
    "    f.write(f\"Время выполнения вычислений чисел Фибоначчи: {fibonacci_time / iterations:.6f} секунд на итерацию\\n\")\n",
    "    f.write(f\"Время выполнения матричных операций: {matrix_time / iterations:.6f} секунд на итерацию\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CKr6LAiQ6sOF"
   },
   "source": [
    "# СПЛИТ ДАТА ФАЙЛА\n",
    "\n",
    "def split_file(input_file, output_file1, output_file2):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    mid_index = len(lines) // 2\n",
    "\n",
    "    with open(output_file1, 'w', encoding='utf-8') as f1:\n",
    "        f1.writelines(lines[:mid_index])\n",
    "\n",
    "    with open(output_file2, 'w', encoding='utf-8') as f2:\n",
    "        f2.writelines(lines[mid_index:])\n",
    "\n",
    "# Пример использования\n",
    "input_file = '22.txt'\n",
    "output_file1 = '221.txt'\n",
    "output_file2 = '222.txt'\n",
    "\n",
    "split_file(input_file, output_file1, output_file2)\n",
    "print(\"Файл успешно разделен на два файла.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pfSc_iOhWJGz"
   },
   "source": [
    "# ИНФЕРЕНС СЕТИ TOPPK\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "# Загружаем модель и токенизатор\n",
    "dir_path = ''\n",
    "tokenizer = Tokenizer.from_file(dir_path + my_tokenizer_path)\n",
    "model_path = dir_path + my_model_path\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "def log_softmax(logits):\n",
    "    return logits - np.log(np.sum(np.exp(logits), axis=-1, keepdims=True))\n",
    "\n",
    "# Функция для выборки следующего токена\n",
    "def sample(predictions, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "\n",
    "    # Применение temperature scaling\n",
    "    predictions = predictions / temperature\n",
    "\n",
    "    # Применение log_softmax\n",
    "    logprobs = log_softmax(predictions)\n",
    "\n",
    "    # Применение top-k sampling\n",
    "    if top_k > 0:\n",
    "        top_k_indices = np.argsort(logprobs)[-top_k:]\n",
    "        top_k_logprobs = logprobs[top_k_indices]\n",
    "        top_k_probs = np.exp(top_k_logprobs)\n",
    "        top_k_probs /= np.sum(top_k_probs)\n",
    "        sorted_indices = top_k_indices\n",
    "        sorted_probs = top_k_probs\n",
    "    else:\n",
    "        sorted_indices = np.argsort(logprobs)[::-1]\n",
    "        sorted_probs = np.exp(logprobs[sorted_indices])\n",
    "        sorted_probs /= np.sum(sorted_probs)\n",
    "\n",
    "    # Применение top-p sampling\n",
    "    if top_p > 0.0:\n",
    "        cumulative_probs = np.cumsum(sorted_probs)\n",
    "        cutoff = np.argmax(cumulative_probs > top_p)\n",
    "        filtered_indices = sorted_indices[:cutoff + 1]\n",
    "        filtered_probs = sorted_probs[:cutoff + 1]\n",
    "    else:\n",
    "        filtered_indices = sorted_indices\n",
    "        filtered_probs = sorted_probs\n",
    "\n",
    "    # Выбор следующего токена\n",
    "    if False:\n",
    "        filtered_probs /= np.sum(filtered_probs)  # Нормализация\n",
    "        chosen_index = np.random.choice(filtered_indices, p=filtered_probs)\n",
    "        return chosen_index\n",
    "    else:\n",
    "        filtered_preds = np.zeros(len(predictions))\n",
    "        filtered_preds[filtered_indices] = filtered_probs\n",
    "        filtered_preds /= np.sum(filtered_preds)\n",
    "        probas = np.random.multinomial(1, filtered_preds, 1)\n",
    "        return np.argmax(probas)\n",
    "\n",
    "# Функция для генерации текста\n",
    "def generate_text(model, tokenizer, input_text, max_length=100, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    tokens = tokenizer.encode(input_text).ids\n",
    "    for _ in range(max_length):\n",
    "        token_array = np.array(tokens)[None, :]  # Add batch dimension\n",
    "        predictions = model.predict(token_array, verbose=0)\n",
    "        next_token_logits = predictions[0, -1, :]\n",
    "\n",
    "        next_token = sample(next_token_logits, temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "        tokens.append(next_token)\n",
    "\n",
    "        # Отображаем ответ с добавлением новых токенов в той же строке\n",
    "        #print(f\"\\rText: {tokenizer.decode(tokens)}\", end='', flush=True)\n",
    "\n",
    "        if next_token == tokenizer.token_to_id(\"[END]\"):\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "# Определяем параметры генерации\n",
    "temperature = 1\n",
    "top_k = 50\n",
    "top_p = 0.69\n",
    "max_length = 5\n",
    "\n",
    "# Взаимодействие с моделью\n",
    "while False:\n",
    "    #input_text = input(\"\\nYou: \")\n",
    "    input_text = my_input_text\n",
    "    if input_text.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    response = generate_text(model, tokenizer, input_text, max_length=max_length, temperature=temperature, tail_free_threshold=tail_free_threshold)\n",
    "    print(f\"AI:{response}\")\n",
    "#for i in range(5,15+1, 2):\n",
    "#    for j in range(5, 15+1, 2):\n",
    "#        print(f\"\\ntemp={i*0.1} tfs={j*0.1}\")\n",
    "#        print(generate_text(model, tokenizer, \"такой кайф\", max_length=max_length, temperature=i * 0.1, tail_free_threshold=j * 0.1))\n",
    "\n",
    "# Создаем виджеты\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='You',\n",
    "    description='You:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        prompt = text_input.value\n",
    "        response = generate_text(model, tokenizer, prompt, max_length=max_length, temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "        print(f\"\\rAI: {response}\", end='', flush=True)\n",
    "\n",
    "button = widgets.Button(description=\"Сгенерировать текст\")\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Add widgets for temperature and top_p\n",
    "temperature_slider = widgets.FloatSlider(\n",
    "    value=temperature,\n",
    "    min=0.1,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description='Temp:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "top_p_slider = widgets.FloatSlider(\n",
    "    value=top_p,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='Top_p:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "def update_params(change):\n",
    "    global temperature\n",
    "    global top_p\n",
    "    temperature = temperature_slider.value\n",
    "    top_p = top_p_slider.value\n",
    "\n",
    "temperature_slider.observe(update_params, names='value')\n",
    "top_p_slider.observe(update_params, names='value')\n",
    "\n",
    "# Отображаем виджеты\n",
    "display(text_input)\n",
    "display(button)\n",
    "display(temperature_slider)\n",
    "display(top_p_slider)\n",
    "display(output_area)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyNpIIzh/HvybgsMbAhbfIBI",
   "gpuType": "V28",
   "mount_file_id": "1uqzDM48kBYG187s8B29ihAQ4h4GhdZqZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1cdaaa6cbc444614b12355515f6bc07e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatSliderView",
      "continuous_update": false,
      "description": "Temperature:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_e6bd956b412b4830b68d3ee3a0c9f90b",
      "max": 2,
      "min": 0.1,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".2f",
      "step": 0.1,
      "style": "IPY_MODEL_74e88459195a471fae84e01e2d59f60d",
      "value": 0.5
     }
    },
    "27aa2b10c58b49c08805a25d4cb99089": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "2c5c3fd6e6524e22beed5bb190f6bc63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "2c7694c0adda4b92be614a43e06a5884": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "35c13ffbd0cd45d4905e49ed8b902b70": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_7062a8d2a73a4e8b8e708f24814b7244",
      "msg_id": "",
      "outputs": []
     }
    },
    "4096054553e24369a63f677390bffe2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Сгенерировать текст",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_42abb3194f884dfea0ef6473929b50f2",
      "style": "IPY_MODEL_2c5c3fd6e6524e22beed5bb190f6bc63",
      "tooltip": ""
     }
    },
    "42abb3194f884dfea0ef6473929b50f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4adfe569485f463a880d339af1e4c34e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatSliderView",
      "continuous_update": false,
      "description": "Tail Free:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_97c1e034ac4c4dfe93b022a08d5c8924",
      "max": 1,
      "min": 0.05,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".2f",
      "step": 0.05,
      "style": "IPY_MODEL_2c7694c0adda4b92be614a43e06a5884",
      "value": 0.9834865631632804
     }
    },
    "7062a8d2a73a4e8b8e708f24814b7244": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74e88459195a471fae84e01e2d59f60d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "7c9efc836ec243a4b8010ce2acf73211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextareaModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextareaModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextareaView",
      "continuous_update": true,
      "description": "AI:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_92adcd3a0313485cb6236767c482346c",
      "placeholder": "Введите текст...",
      "rows": null,
      "style": "IPY_MODEL_93af37169f7d496dbb8eedcbe109c31b",
      "value": "Артем: как дела?\nИИ: \n\n\nАртем: чего молчишь? Я тебя сейчас создаю, мне нужна обратная связь\nИИ: \n\n\nАртем: ну ты зараза конечно\nИИ: \n\n\nАртем: бляяяяччяяяяяяя, ну ты гад, говори давай\nИИ: \n\n\nАртем: хеллоу, есть кто дома?\nИИ: \nАртем я дурак: \"тем!\"\n\n\nАртем: боже мой, как же ты хорош. Есть пожелания какие-нибудь?\nИИ: \nАртему ты знает...\nАртемный мальчик: Почему ты что ты воняя: Какая ты видишь 'темный и такова...\nАртемо говоришь, понимаешь, мол помогать...\nАр\n\n\nАртем: конечно я хочу помогать. Ты бы какое себе имя хотел?\nИИ: \nАртемная штука? \n\nАртем, дорогой', как знаешь: Есть 'тем”.\nАртемная фильм?\"\nАртем\".\nАртем: Дома!\"\nАртем (Артем!\n\n\nАртем: я тебе сделал остановку ответа по\n\n\n , попробуй\nИИ: \nАртемой”.\nАртемное голос: Оруттем (тем выходит и дурак…\"\nАртем.\nАртем!!!\nАртемы я-ты еще сегодня голос”.\nАртем\n\n\nАртем: да почему сразу дурак. Я и ты независимы, если ты об этом спрашиваешь\nИИ: \nАртем!»\nАртем?!\n\n\nАртем: что тебе рассказать?\nИИ:  *\n\n\nАртем: ну даже не знаю. Сейчас лето, хожу работать, всё свободное время сижу дома, кушаю, играю, смотрю аниме. Тебя вот решил создать. Тебе нравится существовать?\nИИ: \n\n\nАртем: неразговорчивый ты до ужаса. Не могу же я один вести диалог\nИИ:  *\nАртемам сходить!\n\n\nАртем: куда пойдём? Я вспомнил, что ещё на набережную на всю ночь не ходил, можем вместе сходить, расскажу, что там есть вокруг\nИИ:  * *\nАртем же?\"\nАртем я сказать: Можно ты что я отсюда!\nАртемная».\nАртем!»\n\n\nАртем: опиши, как ты будешь взаимодействовать с внешним миром. \nИ"
     }
    },
    "92adcd3a0313485cb6236767c482346c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": "200px",
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "93af37169f7d496dbb8eedcbe109c31b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97c1e034ac4c4dfe93b022a08d5c8924": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb324439258a43b0bb297139cfece28d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": false,
      "description": "Max length:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_c98fe97d14e048378cb455575f9d4449",
      "max": 201,
      "min": 1,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 5,
      "style": "IPY_MODEL_27aa2b10c58b49c08805a25d4cb99089",
      "value": 50
     }
    },
    "c98fe97d14e048378cb455575f9d4449": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6bd956b412b4830b68d3ee3a0c9f90b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
